# Desafio MBA Engenharia de Software com IA - Full Cycle

## üìã Sobre o Projeto

Sistema de **ingest√£o e busca sem√¢ntica** utilizando LangChain, PostgreSQL com pgVector e modelos de embeddings (OpenAI ou Google Gemini). O projeto permite:

- **Ingest√£o**: Leitura de arquivos PDF e armazenamento vetorizado no banco de dados
- **Busca**: Interface CLI para fazer perguntas baseadas no conte√∫do do PDF

## üöÄ Tecnologias Utilizadas

- **Python 3.x**
- **LangChain** - Framework para aplica√ß√µes com LLM
- **PostgreSQL + pgVector** - Banco vetorial
- **OpenAI** - Embeddings e LLM
- **Docker & Docker Compose** - Containeriza√ß√£o

## üì¶ Estrutura do Projeto

```
‚îú‚îÄ‚îÄ docker-compose.yml         # Configura√ß√£o do PostgreSQL com pgVector
‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env.example              # Template de vari√°veis de ambiente
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py            # Script de ingest√£o do PDF
‚îÇ   ‚îú‚îÄ‚îÄ search.py            # Script de busca vetorial
‚îÇ   ‚îú‚îÄ‚îÄ chat.py              # CLI interativo
‚îú‚îÄ‚îÄ document.pdf             # PDF para ingest√£o
‚îî‚îÄ‚îÄ README.md                # Este arquivo
```

## ‚öôÔ∏è Pr√©-requisitos

- **Python 3.8+** instalado
- **Docker** e **Docker Compose** instalados
- Chave de API da **OpenAI** ou **Google Gemini**

## üîß Configura√ß√£o e Instala√ß√£o

### 1. Clone o reposit√≥rio

```bash
git clone <url-do-repositorio>
cd mba-ia-desafio-ingestao-busca
```

### 2. Crie o ambiente virtual Python

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

### 3. Instale as depend√™ncias

```bash
pip install -r requirements.txt
```

### 4. Configure as vari√°veis de ambiente

Copie o arquivo de exemplo e edite com suas credenciais:

```bash
cp .env.example .env
```

Edite o arquivo `.env` e configure:

```env
# API Keys
OPENAI_API_KEY=sua-chave-openai-aqui
GOOGLE_API_KEY=sua-chave-google-aqui  # Opcional se usar OpenAI

# Modelos
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
GOOGLE_EMBEDDING_MODEL=models/embedding-001

# Banco de Dados
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/rag

# Configura√ß√µes
PG_VECTOR_COLLECTION_NAME=gpt5_collection
PDF_PATH=./document.pdf
```

**Obtenha suas API Keys:**
- **OpenAI**: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
- **Google Gemini**: [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)

### 5. Adicione seu documento PDF

Coloque o arquivo PDF que deseja ingerir na raiz do projeto com o nome `document.pdf`, ou configure o caminho correto na vari√°vel `PDF_PATH` do arquivo `.env`.

## üéØ Como Executar

### Passo 1: Subir o banco de dados PostgreSQL

```bash
docker compose up -d
```

Verifique se os containers est√£o rodando:
```bash
docker compose ps
```

Voc√™ deve ver os servi√ßos `postgres_rag` e `bootstrap_vector_ext` ativos.

### Passo 2: Executar a ingest√£o do PDF

```bash
python src/ingest.py
```

Este script ir√°:
- Carregar o PDF configurado
- Dividir em chunks de 1000 caracteres com overlap de 150
- Gerar embeddings para cada chunk
- Armazenar no banco vetorial PostgreSQL

**Sa√≠da esperada:**
```
Carregando PDF...
Dividindo em chunks...
Gerando embeddings e salvando no banco...
‚úì Ingest√£o conclu√≠da com sucesso!
```

### Passo 3: Iniciar o chat interativo

```bash
python src/chat.py
```

Agora voc√™ pode fazer perguntas sobre o conte√∫do do PDF:

```
=== Sistema de Busca Sem√¢ntica ===
Digite sua pergunta (ou 'sair' para encerrar):

PERGUNTA: Qual o faturamento da Empresa SuperTechIABrazil?
RESPOSTA: O faturamento foi de 10 milh√µes de reais.

---

PERGUNTA: Quantos clientes temos em 2024?
RESPOSTA: N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta.

---

PERGUNTA: sair
Encerrando...
```

## üîç Como Funciona

### Ingest√£o
1. O PDF √© carregado usando `PyPDFLoader`
2. Texto √© dividido em chunks usando `RecursiveCharacterTextSplitter`
3. Cada chunk √© convertido em embedding usando `OpenAIEmbeddings` ou `GoogleGenerativeAIEmbeddings`
4. Vetores s√£o armazenados no PostgreSQL com `PGVector`

### Busca
1. Pergunta do usu√°rio √© vetorizada
2. Busca por similaridade retorna os 10 chunks mais relevantes (`k=10`)
3. Chunks s√£o concatenados e enviados como contexto para o LLM
4. LLM responde baseado apenas no contexto fornecido

### Prompt Utilizado

```
CONTEXTO:
{resultados concatenados do banco de dados}

REGRAS:
- Responda somente com base no CONTEXTO.
- Se a informa√ß√£o n√£o estiver explicitamente no CONTEXTO, responda:
  "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."
- Nunca invente ou use conhecimento externo.
- Nunca produza opini√µes ou interpreta√ß√µes al√©m do que est√° escrito.

PERGUNTA DO USU√ÅRIO:
{pergunta do usu√°rio}

RESPONDA A "PERGUNTA DO USU√ÅRIO"
```

## üõ†Ô∏è Troubleshooting

### Erro de conex√£o com o banco

Se encontrar erro de conex√£o ao PostgreSQL:

```bash
# Verifique se o container est√° rodando
docker compose ps

# Veja os logs
docker compose logs postgres

# Reinicie os containers
docker compose restart
```

### Erro "ModuleNotFoundError"

Certifique-se de que o ambiente virtual est√° ativado:

```bash
# Linux/Mac
source venv/bin/activate

# Windows
venv\Scripts\activate
```

E que as depend√™ncias foram instaladas:

```bash
pip install -r requirements.txt
```

### Erro de API Key

Verifique se:
- O arquivo `.env` foi criado na raiz do projeto
- A chave de API foi configurada corretamente
- N√£o h√° espa√ßos extras na chave

## üìö Recursos Adicionais

- [Documenta√ß√£o LangChain](https://python.langchain.com/)
- [pgVector GitHub](https://github.com/pgvector/pgvector)
- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Curso de nivelamento LangChain](https://github.com/codeedu/ia-langchain-curso-nivelamento)

## üìù Modelos Utilizados

### OpenAI
- **Embeddings**: `text-embedding-3-small`
- **LLM**: `gpt-4` ou `gpt-3.5-turbo`

### Google Gemini
- **Embeddings**: `models/embedding-001`
- **LLM**: `gemini-2.5-flash-lite` ou `gemini-pro`

## üßπ Limpeza

Para parar e remover os containers:

```bash
docker compose down
```

Para remover tamb√©m os volumes (dados do banco):

```bash
docker compose down -v
```

## üìÑ Licen√ßa

Este projeto foi desenvolvido como parte do MBA em Engenharia de Software com IA - Full Cycle.
